name: Self-Healing Infrastructure CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment'
        required: true
        default: 'dev'
        type: choice
        options: [dev, staging, prod]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}/self-healing-controller
  IMAGE_TAG: ${{ github.sha }}

# ------------------------------------------------
jobs:
  # 1. Quality & Tests for Python + Terraform
  ci:
    name: Lint & Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        tool: [python, terraform]
    steps:
      - uses: actions/checkout@v4

      - name: Cache pip dependencies
        if: matrix.tool == 'python'
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('kubernetes/self-healing/requirements*.txt') }}
      
      - name: Cache Terraform plugins
        if: matrix.tool == 'terraform'
        uses: actions/cache@v3
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-tf-plugins-${{ hashFiles('terraform/**/*.tf') }}

      - name: Setup Python
        if: matrix.tool == 'python'
        uses: actions/setup-python@v4
        with: 
          python-version: '3.9'
      - name: Setup Terraform
        if: matrix.tool == 'terraform'
        uses: hashicorp/setup-terraform@v2
        with: 
          terraform_version: '1.0.11'

      - name: Install dependencies
        if: matrix.tool == 'python'
        run: |
          pip install --upgrade pip
          # Сначала устанавливаем runtime зависимости контроллера
          pip install -r kubernetes/self-healing/requirements.txt
          # Затем dev-зависимости для тестов и линтинга
          pip install -r kubernetes/self-healing/requirements-dev.txt

      - name: Lint with Black & isort & Flake8
        if: matrix.tool == 'python'
        run: |
          black --check kubernetes/self-healing/
          isort --check-only kubernetes/self-healing/
          flake8 kubernetes/self-healing/ --max-line-length=120

      - name: Run unit tests with coverage
        if: matrix.tool == 'python'
        working-directory: kubernetes/self-healing
        run: |
          pytest \
            --maxfail=1 \
            --disable-warnings \
            -v \
            --cov=. \
            --cov-report=xml \
            --cov-report=html
      
      - name: Upload coverage report
        if: matrix.tool == 'python'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: kubernetes/self-healing/htmlcov/
      - name: Configure Terraform plugin cache
        if: matrix.tool == 'terraform'
        run: |
          mkdir -p ~/.terraform.d/plugin-cache
          echo "TF_PLUGIN_CACHE_DIR=$HOME/.terraform.d/plugin-cache" >> $GITHUB_ENV

      - name: Terraform Validate
        if: matrix.tool == 'terraform'
        run: |
          cd terraform
          terraform init
          terraform validate

  # 2. Build & Push Docker
  build-push:
    name: Build & Push Image
    runs-on: ubuntu-latest
    needs: ci
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4
      - uses: docker/setup-buildx-action@v3
      - uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      - uses: docker/metadata-action@v5
        id: meta
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
      - uses: docker/build-push-action@v5
        with:
          context: ./kubernetes/self-healing
          file: ./kubernetes/self-healing/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}

  # 3. Infrastructure: Deploy with Terraform
  infra-deploy:
    name: Terraform Deploy
    runs-on: ubuntu-latest
    needs: build-push
    steps:
      - uses: actions/checkout@v4

      # 3.1 Setup Kubernetes
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      # 3.2 Setup Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: '1.0.11'

      # 3.3 Cache Terraform plugins
      - uses: actions/cache@v3
        with:
          path: ~/.terraform.d/plugin-cache
          key: ${{ runner.os }}-tf-plugins-${{ hashFiles('terraform/**/*.tf') }}

      # 3.4 Configure Terraform plugin cache
      - name: Configure Terraform plugin cache
        run: |
          mkdir -p ~/.terraform.d/plugin-cache
          echo "TF_PLUGIN_CACHE_DIR=$HOME/.terraform.d/plugin-cache" >> $GITHUB_ENV

      # 3.5 Deploy infrastructure
      - name: Terraform Init & Apply
        working-directory: terraform
        run: |
          terraform init
          terraform plan -var="ci_cd_mode=false" -out=tfplan
          terraform apply -auto-approve tfplan

      # 3.6 Ensure we talk to Minikube
      - name: Ensure we talk to Minikube
        run: |
          echo "Current context: $(kubectl config current-context)"
          kubectl cluster-info

      # 3.7 Debug: list namespaces
      - name: Debug list namespaces
        run: |
          kubectl get namespaces

      # 3.8 Create namespaces if missing
      - name: Create namespaces if missing
        run: |
          # Create self-healing namespace if missing
          if ! kubectl get ns self-healing &>/dev/null; then
            echo "❗️ Namespace self-healing not found, creating..."
            kubectl create namespace self-healing
          else
            echo "✅ Namespace self-healing already exists"
          fi
          
          # Create test-app namespace if missing
          if ! kubectl get ns test-app &>/dev/null; then
            echo "❗️ Namespace test-app not found, creating..."
            kubectl create namespace test-app
          else
            echo "✅ Namespace test-app already exists"
          fi

      # 3.9 Wait for deployments to be ready
      - name: Wait for Self-Healing Controller to be ready
        run: |
          echo "🚀 Waiting for Self-Healing Controller to be ready..."
          # Use minikube kubectl to ensure we're using the right context
          minikube kubectl -- rollout status deployment/self-healing-controller \
            -n self-healing \
            --timeout=60s
      
      - name: Wait for test-app to be ready
        run: |
          echo "🚀 Waiting for test-app to be ready..."
          # Use minikube kubectl to ensure we're using the right context
          minikube kubectl -- rollout status deployment/test-app \
            -n test-app \
            --timeout=60s
      
      # 3.7 Debug test-app on failure
      - name: Debug test-app (on failure)
        if: failure()
        run: |
          echo "=== Pods in test-app namespace ==="
          kubectl get pods -n test-app -o wide
          
          echo "=== Describe Deployment ==="
          kubectl describe deployment/test-app -n test-app
          
          echo "=== Describe Pods ==="
          kubectl describe pods -l app=test-app -n test-app
          
          echo "=== Logs from test-app pods ==="
          kubectl logs -l app=test-app -n test-app --tail=50 || echo "No logs available"
          
          echo "=== Events in test-app namespace ==="
          kubectl get events -n test-app --sort-by='.lastTimestamp' | tail -20

      # 3.8 Wait for metrics-server
      - name: Wait for metrics-server
        run: |
          echo "🚀 Waiting for metrics-server to be ready..."
          kubectl wait --for=condition=available deployment/metrics-server -n kube-system --timeout=60s || echo "Metrics-server not ready yet"

      # 3.9 Basic smoke test
      - name: Smoke Test
        run: |
          kubectl get ns self-healing monitoring chaos-engineering
          kubectl get pods --all-namespaces

  # 4. Self-Healing Controller Tests
  self-healing-test:
    name: Self-Healing Controller Tests
    runs-on: ubuntu-latest
    needs: infra-deploy
    steps:
      - uses: actions/checkout@v4
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      - name: Load Docker image
        run: |
          cd kubernetes/self-healing
          docker build -t self-healing-controller:latest .
          minikube image load self-healing-controller:latest

      # Ensure self-healing namespace exists
      - name: Ensure self-healing namespace exists
        run: |
          if ! minikube kubectl -- get namespace self-healing &>/dev/null; then
            echo "⚠️ Namespace 'self-healing' not found—creating it for tests"
            minikube kubectl -- create namespace self-healing
          else
            echo "✅ Namespace 'self-healing' exists"
          fi

      - name: Wait for components to be ready
        run: |
          echo "🚀 Waiting for Self-Healing Controller to roll out..."
          minikube kubectl -- rollout status deployment/self-healing-controller -n self-healing --timeout=120s || echo "Self-Healing Controller deployment not found, skipping"
          
          echo "🚀 Waiting for test-app to roll out..."
          minikube kubectl -- rollout status deployment/test-app -n test-app --timeout=120s || echo "test-app deployment not found, skipping"

      - name: Test Self-Healing Controller health
        run: |
          if minikube kubectl -- get svc self-healing-controller -n self-healing &>/dev/null; then
            # Start port-forward in background
            minikube kubectl -- port-forward -n self-healing svc/self-healing-controller 8081:8080 &
            PF_PID=$!
            sleep 10

            # Test health endpoint
            curl -f http://localhost:8081/health || echo "Health endpoint test failed"
            curl -f http://localhost:8081/metrics || echo "Metrics endpoint test failed"

            kill $PF_PID
          else
            echo "⚠️ Self-Healing Controller service missing, skipping health test"
          fi

      - name: Test pod failure recovery
        run: |
          echo "Creating a failing pod to test self-healing..."
          minikube kubectl -- run test-healing-pod --image=busybox --command -- /bin/sh -c "sleep 3 && exit 1" -n test-app
          sleep 15

          pod_status=$(minikube kubectl -- get pod test-healing-pod -n test-app -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
          if [ "$pod_status" = "NotFound" ]; then
            echo "✅ Self-Healing Controller successfully detected and handled failing pod"
          else
            echo "❌ Self-Healing Controller did not handle failing pod (status: $pod_status)"
            exit 1
          fi

          minikube kubectl -- delete pod test-healing-pod -n test-app --ignore-not-found=true

  # 5. Monitoring Tests
  monitoring-test:
    name: Monitoring Tests
    runs-on: ubuntu-latest
    needs: infra-deploy
    steps:
      - uses: actions/checkout@v4
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      - name: Load Docker image
        run: |
          cd kubernetes/self-healing
          docker build -t self-healing-controller:latest .
          minikube image load self-healing-controller:latest

      # Ensure monitoring namespace exists
      - name: Ensure monitoring namespace
        run: |
          if ! minikube kubectl -- get namespace monitoring &>/dev/null; then
            echo "⚠️ Namespace 'monitoring' not found—creating it for tests"
            minikube kubectl -- create namespace monitoring
          else
            echo "✅ Namespace 'monitoring' exists"
          fi

      - name: Wait for Prometheus stack to roll out
        run: |
          echo "🚀 Waiting for Prometheus rollout..."
          minikube kubectl -- rollout status deployment/prometheus-kube-prometheus-prometheus \
            -n monitoring --timeout=120s || echo "Prometheus deployment not found, skipping"

      - name: Wait for Grafana rollout
        run: |
          echo "🚀 Waiting for Grafana rollout..."
          minikube kubectl -- rollout status deployment/prometheus-grafana \
            -n monitoring --timeout=120s || echo "Grafana deployment not found, skipping"

      - name: Test Prometheus connectivity
        run: |
          if minikube kubectl -- get svc prometheus-kube-prometheus-prometheus -n monitoring &>/dev/null; then
            minikube kubectl -- port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &
            PF_PID=$!
            sleep 5
            curl -f http://localhost:9090/api/v1/query?query=up || echo "Prometheus connectivity test failed"
            curl -f "http://localhost:9090/api/v1/query?query=up{job=\"self-healing-controller\"}" || echo "No self-healing metrics yet"
            kill $PF_PID
          else
            echo "⚠️ Prometheus service missing, skipping connectivity test"
          fi

      - name: Test Grafana connectivity
        run: |
          if minikube kubectl -- get svc prometheus-grafana -n monitoring &>/dev/null; then
            minikube kubectl -- port-forward -n monitoring svc/prometheus-grafana 3000:3000 &
            PF_PID=$!
            sleep 5
            curl -f http://localhost:3000/api/health || echo "Grafana connectivity test failed"
            kill $PF_PID
          else
            echo "⚠️ Grafana service missing, skipping connectivity test"
          fi

  # 6. Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [self-healing-test, monitoring-test]
    steps:
      - uses: actions/checkout@v4
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      # 1) Убедимся, что namespace test-app есть
      - name: Ensure test-app namespace exists
        run: |
          if ! minikube kubectl -- get namespace test-app &>/dev/null; then
            echo "⚠️ Namespace 'test-app' not found—creating it for integration tests"
            minikube kubectl -- create namespace test-app
          else
            echo "✅ Namespace 'test-app' exists"
          fi

      # 2) Проверяем Kured
      - name: Test Kured integration
        run: |
          echo "🔍 Checking Kured in kured namespace..."
          if minikube kubectl -- get namespace kured &>/dev/null; then
            if minikube kubectl -- get pods -n kured | grep Running; then
              echo "✅ Kured is running"
            else
              echo "⚠️ No Running Kured pods found"
            fi
            minikube kubectl -- describe daemonset kured -n kured || echo "Kured daemonset not found"
          else
            echo "⚠️ Namespace 'kured' not found—skipping Kured tests"
          fi

      # 3) Проверяем PrometheusRules / Alertmanager
      - name: Test monitoring alerts
        run: |
          echo "🔍 Checking PrometheusRules and Alertmanager config..."
          # PrometheusRules CRD может не быть в кластере, поэтому guard-clause
          if minikube kubectl -- api-resources | grep -q prometheusrules; then
            minikube kubectl -- get prometheusrules -n monitoring || echo "No PrometheusRules found"
          else
            echo "⚠️ CRD 'prometheusrules' not present—skipping"
          fi
          # Alertmanager ConfigMap
          if minikube kubectl -- get namespace monitoring &>/dev/null; then
            minikube kubectl -- get configmap -n monitoring | grep alertmanager || echo "No Alertmanager config found"
          else
            echo "⚠️ Namespace 'monitoring' not found—skipping Alertmanager config test"
          fi

      # 4) Проверяем test-app
      - name: Test test application
        run: |
          echo "🔍 Checking test-app pods in test-app namespace..."
          if minikube kubectl -- get namespace test-app &>/dev/null; then
            if minikube kubectl -- get pods -n test-app | grep Running; then
              echo "✅ test-app pods are running"
            else
              echo "❌ No Running pods in test-app namespace"
            fi
          else
            echo "⚠️ Namespace 'test-app' not found—skipping test-app checks"
          fi

      - name: Test application accessibility
        run: |
          echo "🔍 Testing test-app service..."
          if minikube kubectl -- get svc test-app -n test-app &>/dev/null; then
            minikube kubectl -- port-forward -n test-app svc/test-app 8080:80 &
            PF_PID=$!
            sleep 5
            if curl -sf http://localhost:8080; then
              echo "✅ test-app service is accessible"
            else
              echo "❌ test-app service not accessible"
            fi
            kill $PF_PID
          else
            echo "⚠️ Service 'test-app' not found—skipping accessibility test"
          fi

      - name: Test HPA functionality
        run: |
          echo "🔍 Checking HPA in test-app namespace..."
          if minikube kubectl -- get hpa -n test-app &>/dev/null; then
            minikube kubectl -- describe hpa test-app-hpa -n test-app || echo "HPA exists but describe failed"
          else
            echo "⚠️ HPA 'test-app-hpa' not found—skipping HPA checks"
          fi

  # 7. Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    steps:
      - uses: actions/checkout@v4
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      - name: Test resource limits
        run: |
          minikube kubectl -- describe nodes
          minikube kubectl -- top nodes || echo "Metrics server not available"

      - name: Test scalability
        run: |
          minikube kubectl -- scale deployment test-app -n test-app --replicas=5
          minikube kubectl -- wait --for=condition=available deployment/test-app -n test-app --timeout=300s

          running_pods=$(minikube kubectl -- get pods -n test-app | grep Running | wc -l)
          echo "Running pods: $running_pods"

          if [ "$running_pods" -ge 3 ]; then
            echo "✅ Scaling test passed"
          else
            echo "❌ Scaling test failed"
            exit 1
          fi

      - name: Test multiple pod failures
        run: |
          echo "Testing multiple pod failures..."

          for i in {1..3}; do
            minikube kubectl -- run test-fail-$i --image=busybox --command -- /bin/sh -c "sleep 2 && exit 1" -n test-app
          done

          sleep 20

          remaining_pods=$(minikube kubectl -- get pods -n test-app | grep test-fail | wc -l)
          echo "Remaining test pods: $remaining_pods"

          if [ "$remaining_pods" -eq 0 ]; then
            echo "✅ Multiple pod failure test passed"
          else
            echo "❌ Multiple pod failure test failed"
            exit 1
          fi

  # 8. Cleanup & Report
  cleanup:
    name: Cleanup & Report
    runs-on: ubuntu-latest
    needs: [performance-tests]
    if: always()
    steps:
      - uses: actions/checkout@v4
      - name: Setup Minikube
        uses: ./.github/actions/setup-minikube

      - name: Collect logs
        run: |
          minikube kubectl -- get pods --all-namespaces >> system-report.txt
          minikube kubectl -- logs -n self-healing deployment/self-healing-controller --tail=50 >> system-report.txt || echo "No self-healing logs available" >> system-report.txt
          minikube kubectl -- get events --all-namespaces --sort-by='.lastTimestamp' | tail -20 >> system-report.txt

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: system-report
          path: system-report.txt

      - name: Cleanup test resources
        if: always()
        run: |
          minikube kubectl -- delete pod -l test=true -n test-app --ignore-not-found=true
          minikube kubectl -- delete pod test-healing-pod -n test-app --ignore-not-found=true
          minikube kubectl -- delete pod test-crash-pod -n test-app --ignore-not-found=true
